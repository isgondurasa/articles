[{"id":0,"href":"/posts/1_distributed_transactions/","title":"Синхронизация данных при переходе с монолита на микросервисы","section":"Posts","content":"Синхронизация данных при переходе с монолита на микросервисы# Недавно мне пришлось заниматься миграцией монолитной системы для управления медиаконтентом в микросервисную архитектуру. Причины перехода оставлю за скобками - это стандартная история про устаревший монолит, который сложно поддерживать и масштабировать. Сосредоточусь на технических аспектах. В статье расскажу о решении одной из ключевых задач при переходе от монолита к микросервисам - синхронизации данных между старой и новой системами.\nОписание исходной системы# Для начала определимся с терминологией. Под \u0026ldquo;старой системой\u0026rdquo; я имею в виду исходное монолитное приложение, под \u0026ldquo;новой системой\u0026rdquo; - новое микросервисное решение, к которому мы мигрируем. На диаграмме ниже показана исходная архитектура:\nC4Container title Монолитная система управления контентом (Container View) UpdateLayoutConfig($c4ShapeInRow=\u0026#34;2\u0026#34;, $c4BoundaryInRow=\u0026#34;4\u0026#34;) Person(Manager, \u0026#34;Контент-менеджер\u0026#34;, \u0026#34;Создает и редактирует контент\u0026#34;) Person(Customer, \u0026#34;Потребитель контента\u0026#34;, \u0026#34;Просматривает контент\u0026#34;) Container_Boundary(monolith, \u0026#34;Монолитное приложение\u0026#34;) { Container(AdminUI, \u0026#34;Admin UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Управление и ввод контента\u0026#34;) Container(ClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(Api, \u0026#34;Application Core\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Бизнес-логика\u0026#34;) ContainerDb(DB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Структурированные данные\u0026#34;) ContainerDb(MediaStorage, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) } Rel(Manager, AdminUI, \u0026#34;Работает через браузер\u0026#34;) Rel(Customer, ClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(AdminUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(ClientUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(Api, DB, \u0026#34;Чтение / запись\u0026#34;) Rel(Api, MediaStorage, \u0026#34;Загрузка / получение файлов\u0026#34;)Старая система представляла собой классический монолит: единое приложение, которое отвечало за ввод, хранение, обработку и отображение контента. Контент - это совокупность текста и медиаданных (звуковые файлы, видео файлы, картинки). Система была создана достаточно давно, архитектура и технологический стек морально устарели, что начало проявляться в сложностях с сопровождением, масштабирование и написанием новых фич. Также были сложности с поиском специалистов, которые были бы готовы это приложение поддерживать.\nФункционально монолит представлял два основных интерфейса:\nАдминистративный интерфейс - ввод и редактирование текстовых данных и медиаданных, с возможностью модерации и валидации; Клиентский интерфейс - отображение контента конечным пользователям, включая персонализацию и рекомендации. Постановка задачи# Бизнесу было важно получить результат в сжатые сроки, поэтому вариант \u0026ldquo;построить новую систему целиком и затем переключить трафик\u0026rdquo; не подходил. В таких случаях хорошо работает паттерн Strangler Fig, описанный Мартином Фаулером: функциональность старой системы постепенно заменяется новой, работающей в реальных условиях.\nКонтент создавался исключительно через старую систему, и та же самая старая система предоставляла контент для чтения конечным пользователям. Для начала требовалось понять, а какую часть стоит заменить вначале, а что потом. С помощью CQRS можно отделить код для создания контента от кода для его чтения. Соответственно, самый очевидный первый шаг - это логически декомпозировать монолит на read-side и write-side, соответственно. Так как бизнес ориентирован на пользователей, соответственно выгодно переходить на более современные технологии UI, очевидно, что первым шагом требуется переделать read-side часть. Это логично: читающая часть обычно проще, лучше масштабируется и менее чувствительна к временной неконсистентности. Пользователь может увидеть слегка устаревший контент, но это лучше, чем полный отказ. В короткие сроки был создан новый UI, бекенд, подготовлены базы данных. Осталось решить проблему с передачей данных из старой системы в новую. Это типичный сценарий эволюционной миграции: старая система продолжает обслуживать пользователей, параллельно разрабатывается новая, и между ними требуется надежная синхронизация данных.\nКлючевой вопрос - как обеспечить эту синхронизацию без радикального переписывания legacy-кода в монолите и без потери данных.\nНа следующем этапе планировалась декомпозиция на микросервисы write-side - административной части и процессов создания контента. Рассказ про переделку write-side выходит за рамки статьи. Возможно в дальнейшем я напишу статью про переделку этой части приложения.\nНиже показан расширенный контекст системы, включающий старую и новую части.\nC4Container title Контекст старой и новой системы Person(Manager, \u0026#34;Контент-менеджер\u0026#34;, \u0026#34;Создает и редактирует контент\u0026#34;) Person(Customer, \u0026#34;Потребитель контента\u0026#34;, \u0026#34;Просматривает контент\u0026#34;) Container_Boundary(monolith, \u0026#34;Монолитное приложение\u0026#34;) { Container(AdminUI, \u0026#34;Admin UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Управление и ввод контента\u0026#34;) Container(ClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(Api, \u0026#34;Application Core\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Бизнес-логика\u0026#34;) ContainerDb(DB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Структурированные данные\u0026#34;) ContainerDb(MediaStorage, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) } Rel(Manager, AdminUI, \u0026#34;Работает через браузер\u0026#34;) Rel(Customer, ClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(AdminUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(ClientUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(Api, DB, \u0026#34;Чтение / запись\u0026#34;) Rel(Api, MediaStorage, \u0026#34;Загрузка / получение файлов\u0026#34;) Container_Boundary(b1, \u0026#34;Новый интерфейс\u0026#34;) { Container(NewClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(NewBackend, \u0026#34;Application Core\u0026#34;, \u0026#34;Новый бекенд\u0026#34;, \u0026#34;Бизнес-логика\u0026#34;) ContainerDb(NewSystemDB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Структурированные данные\u0026#34;) ContainerDb(ElasticDB, \u0026#34;Elastic DB\u0026#34;, \u0026#34;Elastic\u0026#34;, \u0026#34;Данные для быстрого поиска\u0026#34;) ContainerDb(NewSystemDBS3, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) } Rel(Customer, NewClientUI, \u0026#34;Работает через браузер\u0026#34;) UpdateLayoutConfig($c4ShapeInRow=\u0026#34;2\u0026#34;, $c4BoundaryInRow=\u0026#34;2\u0026#34;)На диаграмме описаны две системы: старая и новая. Предполагалось, что старая система должна работать как и прежде, но при этом она должна каким-то образом отправлять данные в новую. Вся задача сводится к консистентной и надежной передаче данных из одной системы в другую. Важно понимать, две вещи: вероятен временной лаг между созданием контента в админке и предоставлением его конечному пользователю, а также что админка в монолите должна являться единственным источником истины. Если контент-менеджеры начнут создавать или менять данные одновременно через старую и новую системы, возникнет рассинхронизация. В худшем случае это может привести к проблемам совместного редактирования (смотри OT и CRDT), так что лучше этого избегать.\nТакже хочу отметить, что механизм синхронизации, о котором пойдет речь в статье, учитывает только новые данные, которые создаются или редактируются через интерфейс админки старой системы. Задача миграции исторических данных - это отдельная тема, которая выходит за рамки этой статьи. Для исторических данных обычно используются батч-процессы, ETL-инструменты (например, Apache Airflow), с валидацией на консистентность перед переключением.\nНе стоит забывать, что конечная цель - переделка всей старой системы, то есть после полного перевода на современную архитектуры всей системы, механизм синхронизации предполагается деактивировать за ненадобностью.\nНиже представлены различные варианты реализации, как можно реализовать механизм синхронизации данных. Каждый из подходов имеет свои преимущества и недостатки, и выбор зависит от конкретных требований проекта:\nДвухфазный коммит (2PC) - классический подход для распределенных транзакций Saga - современный паттерн для длинных распределенных транзакций Temporal Workflow - специализированная платформа для управления бизнес-процессами Оркестратор через Event Sourcing - комбинация event-driven архитектуры и оркестрации Outbox Pattern - надежная доставка событий из монолита Change Data Capture (CDC) - автоматическое отслеживание изменений в БД Варианты реализации# Перед тем как перейти к описанию конкретных подходов, важно понять критерии выбора. Не существует универсального решения, которое подходит для всех случаев. Каждый проект имеет свои особенности, ограничения и требования.\nЯ сформулировал следующие вопросы, которые помогают выбрать правильный подход:\nНасколько быстро требуется передать данные конечному клиенту? Нужна ли синхронизация в реальном времени или допустима задержка в секунды/минуты? Какая допустимая задержка между системами? Синхронная передача (миллисекунды) или асинхронная (секунды/минуты)? Требуется ли строгая консистентность данных между системами? Должны ли данные в обеих системах быть идентичными в любой момент времени, или допустима eventual consistency? Какова допустимая сложность реализации? Есть ли время и ресурсы на сложные решения или нужен быстрый MVP? Какой объем данных передается? Малые порции данных или большие батчи? Влияет на выбор механизма передачи. Как часто происходят изменения? Постоянный поток событий или периодические обновления? Какая ожидается нагрузка? Низкая, средняя или высокая? Определяет требования к производительности решения. Ниже я разберу каждый подход отдельно, чтобы понять, когда какой использовать.\n1. Двухфазный коммит (2PC)# Суть подхода: Классический протокол распределенных транзакций, который гарантирует атомарность операций в нескольких системах. Это один из старейших подходов, описанный еще в 1970-х годах. Подробное описание можно найти тут.\nХотя он устарел для большинства современных сценариев, он все еще применяется в нишевых случаях, таких как финансовые системы с поддержкой XA-стандарта.\nКак работает:\nПротокол состоит из двух фаз:\nФаза 1 (Prepare): Координатор запрашивает у всех участников готовность к коммиту. Каждый участник блокирует необходимые ресурсы и отвечает \u0026ldquo;готов\u0026rdquo; или \u0026ldquo;не готов\u0026rdquo;. Важно понимать, что на этом этапе изменения еще не фиксируются, но ресурсы уже заблокированы.\nФаза 2 (Commit/Rollback): Если все участники ответили \u0026ldquo;готов\u0026rdquo;, координатор отправляет команду на коммит. Если хотя бы один участник ответил \u0026ldquo;не готов\u0026rdquo; или произошел сбой, координатор отправляет команду на откат всех изменений.\nЭто гарантирует, что либо все системы обновятся одновременно, либо ни одна не изменится - классическая ACID транзакция, но распределенная.\nПлюсы:\nСтрогая консистентность данных - данные в обеих системах всегда синхронизированы, что критично для сценариев вроде банковских переводов. Атомарность операций - либо все успешно, либо ничего не изменилось. Простая концепция для понимания - логика интуитивно понятна для разработчиков с опытом работы с ACID транзакциями. Минусы:\nБлокировка ресурсов на время транзакции - это главная проблема производительности. Пока транзакция не завершится, ресурсы заблокированы, что может привести к дедлокам и снижению пропускной способности системы. Координатор - единая точка отказа - если координатор упадет в неподходящий момент, транзакция может \u0026ldquo;зависнуть\u0026rdquo; в неопределенном состоянии. Требуется настройка высокой доступности. Не подходит для высоконагруженных систем - блокировки делают этот подход непригодным для систем с высокой нагрузкой. Сложность отката при частичных сбоях - если один из участников упал после фазы Prepare, нужно разбираться, что делать с остальными. Это требует ручного вмешательства или сложных механизмов восстановления. Когда использовать:\nКогда критична строгая консистентность и нельзя допустить рассинхронизации данных даже на секунды. Низкая нагрузка на систему - когда транзакций немного и они короткие. Короткие транзакции - чем короче транзакция, тем меньше времени ресурсы заблокированы. Когда НЕ использовать:\nВ высоконагруженных системах. Когда транзакции могут быть долгими (более нескольких секунд). В микросервисной архитектуре с множеством сервисов, где предпочтительны асинхронные паттерны. Честно говоря, в современных сложных и распределенных системах двухфазный коммит практически не используется, так что он приведен здесь скорее как исторический пример.\n2. Saga - распределенная транзакция# Суть подхода: Длинная транзакция разбивается на последовательность локальных транзакций, каждая из которых имеет компенсирующее действие (compensating transaction). Это современная альтернатива 2PC, которая решает проблему блокировок за счет ослабления требований к консистентности. Подробное описание можно найти тут.\nSaga идеальна для микросервисов, где каждый сервис отвечает за свою часть процесса. Паттерн поддерживает два механизма работы:\nХореография - когда каждый сервис понимает, что он должен делать в каждый момент времени. Оркестрация - когда присутствует центральный координатор, который управляет транзакциями. Подробнее описано ниже. На практике Saga часто интегрируется с message broker\u0026rsquo;ами вроде Kafka для обеспечения доставки событий и мониторится через distributed tracing.\n2.1. Хореография# В этом подходе каждый сервис знает, что делать дальше и публикует события для следующих шагов. Преимущество этого подхода в том, что нет центрального координатора, что делает систему более отказоустойчивой. Недостаток - сложнее отслеживать общий поток выполнения и отлаживать проблемы. Для SRE это означает необходимость в хорошем логировании и correlation ID для трассировки.\nУпрощенная диаграмма последовательности представлена ниже:\nsequenceDiagram participant A as Старая система participant B as Новая система participant E as Event Bus A-\u0026gt;\u0026gt;E: Событие \u0026#34;Контент создан\u0026#34; E-\u0026gt;\u0026gt;B: Уведомление о событии B-\u0026gt;\u0026gt;B: Сохранение данных B-\u0026gt;\u0026gt;E: Событие \u0026#34;Данные сохранены\u0026#34; E-\u0026gt;\u0026gt;A: Подтверждение2.2. Оркестрация# В отличие от хореографии, здесь есть центральный оркестратор, который управляет последовательностью шагов. Мы добавляем в схему единую точку отказа, но в ответ получаем контроль за всем процессом миграции. Преимущество - централизованное управление и сравнительная простота в отслеживании состояния всего процесса.\nsequenceDiagram participant O as Оркестратор participant A as Старая система participant B as Новая система A-\u0026gt;\u0026gt;O: Запрос на синхронизацию O-\u0026gt;\u0026gt;A: Блокировка данных O-\u0026gt;\u0026gt;B: Передача данных B-\u0026gt;\u0026gt;O: Подтверждение O-\u0026gt;\u0026gt;A: РазблокировкаНа диаграмме ниже представлен вариант реализации подобной архитектуры.\nC4Container title Контекст старой и новой системы с Saga Person(Manager, \u0026#34;Контент-менеджер\u0026#34;, \u0026#34;Создает и редактирует контент\u0026#34;) Person(Customer, \u0026#34;Потребитель контента\u0026#34;, \u0026#34;Просматривает контент\u0026#34;) Container_Boundary(monolith, \u0026#34;Монолитное приложение\u0026#34;) { Container(AdminUI, \u0026#34;Admin UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Управление и ввод контента\u0026#34;) Container(ClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(Api, \u0026#34;Application Core\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Write-side бизнес-логика\u0026#34;) ContainerDb(DB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Структурированные данные\u0026#34;) ContainerDb(MediaStorage, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) ContainerDb(Outbox, \u0026#34;Outbox Table\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Доменные события\u0026#34;) } Rel(Manager, AdminUI, \u0026#34;Работает через браузер\u0026#34;) Rel(Customer, ClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(AdminUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(ClientUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(Api, DB, \u0026#34;Чтение / запись\u0026#34;) Rel(Api, MediaStorage, \u0026#34;Загрузка / получение файлов\u0026#34;) Rel(Api, Outbox, \u0026#34;Записывает события\\nв одной транзакции\u0026#34;) Container(EventBus, \u0026#34;Message Broker\u0026#34;, \u0026#34;Kafka / RabbitMQ\u0026#34;, \u0026#34;Доставка событий\u0026#34;) Rel(Outbox, EventBus, \u0026#34;Публикация событий \\n at-least-once\u0026#34;) Container_Boundary(b1, \u0026#34;Новая система\u0026#34;) { Container(NewClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(NewBackend, \u0026#34;Application Core\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Read-side логика\u0026#34;) Container(SagaConsumer, \u0026#34;Saga Participant\u0026#34;, \u0026#34;Service\u0026#34;, \u0026#34;Реакция на события, локальные транзакции, компенсации\u0026#34;) ContainerDb(NewDB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Проекции данных\u0026#34;) ContainerDb(ElasticDB, \u0026#34;Elastic DB\u0026#34;, \u0026#34;Elasticsearch\u0026#34;, \u0026#34;Поиск\u0026#34;) ContainerDb(NewDBS3, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) } Rel(Customer, NewClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(NewClientUI, NewBackend, \u0026#34;HTTP / JSON\u0026#34;) Rel(EventBus, SagaConsumer, \u0026#34;Доменные события\u0026#34;) Rel(SagaConsumer, ElasticDB, \u0026#34;Обновление индексов\u0026#34;) Rel(SagaConsumer, NewDBS3, \u0026#34;Копирование медиа\u0026#34;) Rel(SagaConsumer, NewDB, \u0026#34;Копирование-откат данных\u0026#34;) UpdateLayoutConfig($c4ShapeInRow=\u0026#34;3\u0026#34;, $c4BoundaryInRow=\u0026#34;2\u0026#34;)Плюсы:\nВысокая производительность - нет долгих блокировок, каждая транзакция локальна и быстра (latency \u0026lt;1sec per step). Отказоустойчивость - если один шаг упал, можно выполнить компенсирующую транзакцию для предыдущих шагов; интегрируйте с circuit breakers (Hystrix/Resilience4j). Масштабируемость - каждый сервис может масштабироваться независимо, с backpressure для обработки пиков. Подходит для микросервисов - это один из стандартных паттернов для микросервисной архитектуры, с поддержкой в фреймворках вроде Spring Boot. Минусы:\nСложность реализации компенсирующих транзакций - нужно продумать, как реализовать откат каждого шага (например, для необратимых действий вроде email - использовать \u0026ldquo;undo\u0026rdquo; или лог). Eventual consistency - данные могут быть несинхронизированы некоторое время. Это фундаментальный компромисс этого подхода, с возможной сильной консистентностью в ключевых шагах. Сложность отладки распределенных транзакций - когда что-то пошло не так, нужно собирать логи из разных сервисов; используйте ELK stack. Нужна инфраструктура для событий - требуется message broker (Kafka, RabbitMQ и т.д.), с cost of ownership (maintenance, partitioning). Когда использовать:\nДлинные бизнес-процессы - когда транзакция может занимать минуты или даже часы (например, обработка видео). Высокая нагрузка - когда нужно обрабатывать много транзакций параллельно. Готова принять eventual consistency - когда допустимо, что данные будут синхронизированы не мгновенно, а через некоторое время. Когда НЕ использовать:\nКогда требуется строгая консистентность в реальном времени. Когда компенсирующие транзакции слишком сложны или невозможны (например, для внешних API). 3. Temporal Workflow# Суть подхода: Использование специализированной платформы для управления долгоживущими бизнес-процессами с гарантиями выполнения. Temporal - это open-source платформа, созданная бывшими инженерами Uber, которая решает проблему надежного выполнения распределенных бизнес-процессов. Она обеспечивает детерминизм: workflow-код выполняется как последовательность шагов, где состояние сохраняется в durable storage (Cassandra или PostgreSQL), позволяя восстановление после сбоев.\nC4Container title Контекст старой и новой системы с Temporal Workflow Person(Manager, \u0026#34;Контент-менеджер\u0026#34;, \u0026#34;Создает и редактирует контент\u0026#34;) Person(Customer, \u0026#34;Потребитель контента\u0026#34;, \u0026#34;Просматривает контент\u0026#34;) Container_Boundary(monolith, \u0026#34;Монолитное приложение\u0026#34;) { Container(AdminUI, \u0026#34;Admin UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Управление и ввод контента\u0026#34;) Container(ClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(Api, \u0026#34;Application Core\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Write-side бизнес-логика\u0026#34;) ContainerDb(DB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Источник истины\u0026#34;) ContainerDb(MediaStorage, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) ContainerDb(Outbox, \u0026#34;Outbox Table\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Доменные события\u0026#34;) } Rel(Manager, AdminUI, \u0026#34;Работает через браузер\u0026#34;) Rel(Customer, ClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(AdminUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(ClientUI, Api, \u0026#34;HTTP / JSON\u0026#34;) Rel(Api, DB, \u0026#34;Чтение / запись\u0026#34;) Rel(Api, MediaStorage, \u0026#34;Загрузка / получение файлов\u0026#34;) Rel(Api, Outbox, \u0026#34;Фиксация событий\\nв одной транзакции\u0026#34;) Container(EventBus, \u0026#34;Message Broker\u0026#34;, \u0026#34;Kafka / RabbitMQ\u0026#34;, \u0026#34;Доставка событий\u0026#34;) Rel(Outbox, EventBus, \u0026#34;Публикация событий at-least-once\u0026#34;) Container_Boundary(temporal, \u0026#34;Temporal Platform\u0026#34;) { Container(TemporalServer, \u0026#34;Temporal Server\u0026#34;, \u0026#34;Workflow Engine\u0026#34;, \u0026#34;Хранение состояния workflow,\\nretry, таймеры\u0026#34;) Container(TemporalWorker, \u0026#34;Temporal Worker\u0026#34;, \u0026#34;Worker\u0026#34;, \u0026#34;Оркестрация read-side\\nпроцессов\u0026#34;) } Rel(EventBus, TemporalWorker, \u0026#34;Триггер workflow\\n событие\u0026#34;) Rel(TemporalWorker, TemporalServer, \u0026#34;Poll / Execute Activities\u0026#34;) Container_Boundary(b1, \u0026#34;Новая система\u0026#34;) { Container(NewClientUI, \u0026#34;Client UI\u0026#34;, \u0026#34;Web UI\u0026#34;, \u0026#34;Отображение контента\u0026#34;) Container(NewBackend, \u0026#34;Read API\u0026#34;, \u0026#34;Backend\u0026#34;, \u0026#34;Read-side логика\u0026#34;) ContainerDb(NewSystemDB, \u0026#34;Основная БД\u0026#34;, \u0026#34;RDBMS\u0026#34;, \u0026#34;Проекции данных\u0026#34;) ContainerDb(ElasticDB, \u0026#34;Elastic DB\u0026#34;, \u0026#34;Elasticsearch\u0026#34;, \u0026#34;Поиск\u0026#34;) ContainerDb(NewSystemDBS3, \u0026#34;S3-хранилище\u0026#34;, \u0026#34;Object Storage\u0026#34;, \u0026#34;Медиафайлы\u0026#34;) } Rel(Customer, NewClientUI, \u0026#34;Работает через браузер\u0026#34;) Rel(NewClientUI, NewBackend, \u0026#34;HTTP / JSON\u0026#34;) Rel(TemporalWorker, NewBackend, \u0026#34;Activity calls\\n HTTP / gRPC\u0026#34;) Rel(NewBackend, NewSystemDB, \u0026#34;Запись проекций\u0026#34;) Rel(NewBackend, ElasticDB, \u0026#34;Обновление индексов\u0026#34;) Rel(NewBackend, NewSystemDBS3, \u0026#34;Копирование медиа\u0026#34;) UpdateLayoutConfig($c4ShapeInRow=\u0026#34;3\u0026#34;, $c4BoundaryInRow=\u0026#34;3\u0026#34;)Как работает:\nWorkflow описывает бизнес-логику как код - пишется код, который описывает последовательность шагов. Temporal обеспечивает запуск этого кода на worker\u0026rsquo;ах. Temporal гарантирует выполнение workflow даже при сбоях - если сервис упал, Temporal восстановит выполнение с последней успешной точки. Это достигается за счет того, что каждое действие workflow детерминировано и может быть воспроизведено из event history. Например, если шаг \u0026ldquo;мигрировать видео\u0026rdquo; прерван, Temporal replay\u0026rsquo;нет историю и продолжит. Автоматические retry и таймауты - можно настроить политики повторных попыток (exponential backoff) и таймауты для каждого шага workflow. Для медиа - retry для загрузки файлов с лимитом 5 попыток. Визуализация выполнения workflow - Temporal предоставляет UI, где можно видеть состояние всех запущенных workflow, их историю выполнения и отлаживать проблемы. Интеграция с Prometheus для метрик (completion rate, latency percentiles). Плюсы:\nгарантия доставки - at-least-once с автоматическим recovery. Простота написания бизнес-логики. Встроенные механизмы ретраев и таймаутов. UI для мониторинга. Поддержка долгоживущих процессов (дни, недели). Минусы:\nДополнительная инфраструктура. Может быть избыточно для простых случаев. Когда использовать:\nСложные долгоживущие бизнес-процессы - миграция медиа с несколькими шагами (текст, видео, аудио). Критична надежность выполнения - когда сбои недопустимы. Нужна хорошая наблюдаемость - для команд SRE с фокусом на мониторинг. 4. Event Sourcing# Суть подхода: Комбинация паттерна Event Sourcing (хранение событий вместо состояния) и оркестратора для управления синхронизацией. Это более сложный, но мощный подход, который дает полную историю всех изменений в системе. Event Sourcing интегрируется с оркестратором (например, Conductor от Netflix) для координации.\nКак работает: Event Sourcing - это паттерн, при котором вместо хранения текущего состояния объекта мы храним последовательность событий, которые привели к этому состоянию. Текущее состояние можно восстановить, \u0026ldquo;проиграв\u0026rdquo; все события с начала. Это дает множество преимуществ: полный аудит, возможность \u0026ldquo;перемотать\u0026rdquo; время назад, создание новых представлений данных и т.д. Для оптимизации используйте snapshots - периодические дампы состояния, чтобы избежать полного replay (например, snapshot каждые 100 событий).\nОпишем объект, который хотим мигрировать из старой системы в новую, как конечный автомат (FSM - Finite State Machine). FSM может быть расширен параллельными ветками, например, для одновременной миграции текста и видео.\nstateDiagram-v2 [*] --\u0026gt; Text_Migrated Text_Migrated --\u0026gt; Error_Happened Text_Migrated --\u0026gt; Video_Migrated Video_Migrated --\u0026gt; Error_Happened Video_Migrated --\u0026gt; Audio_Migrated Audio_Migrated --\u0026gt; Migrate_Commited Audio_Migrated --\u0026gt; Error_Happened Migrate_Commited --\u0026gt; [*] Error_Happened --\u0026gt; Text_Migrated: fix error Error_Happened --\u0026gt; Video_Migrated: fix error Error_Happened --\u0026gt; Audio_Migrated: fix errorЭто только один из вариантов FSM, который можно реализовать.\nБолее подробно про реализацию Event Sourcing можно почитать тут. Особенность Event Sourcing заключается в том, что последний актуальный контекст можно всегда восстановить по записанным в базу событиям.\nПлюсы:\nВсегда есть возможность посмотреть полную историю изменений Возможность восстановления состояния - полезно для дебага. Cервисы реагируют на события независимо. Минусы:\nСложность реализации - надо перестроиться от хранения конечного состояния в базе к хранению последовательности событий. Требуется хранилище для событий - большой оверхед память для хранения данных. Когда использовать:\nНужна полная история изменений. Если требуется использовать FSM (например область интернет платежей) Требуется возможность восстановления состояния Критичен аудит 5. Outbox Pattern# Суть подхода: Паттерн для надежной доставки событий из монолита без изменения основной логики. Это один из самых популярных паттернов для обеспечения межсервисного взаимодействия, потому что он требует минимальных изменений в существующем коде. Для медиа - отдельные outbox\u0026rsquo;ы для типов данных.\nЧто если мы параллельно будем отправлять данные, используя outbox паттерн? Это позволит гарантировать доставку событий, не нарушая работу существующей системы.\nflowchart LR A(Старая система) B(Новая система) text(Text Outbox) audio(Audio Outbox) video(Video Outbox) A --\u0026gt; text A --\u0026gt; audio A --\u0026gt; video text --\u0026gt; B audio --\u0026gt; B video --\u0026gt; BДостаточно простая архитектура взаимодействия. Использование паттерна дает гарантию at-least-once, что означает, что событие точно будет доставлено, но может быть доставлено несколько раз. Это можно обработать и дедуплицировать на принимающей стороне, используя идемпотентные ключи. В моей голове гарантия at-least-once лучше, чем at-most-once , потому что потеря события хуже, чем его дублирование. Дублирование можно обработать и дедуплицировать, а потерянное событие может привести к рассинхронизации данных.\nКак работает:\nМеханизм работы прост и элегантен:\nПри сохранении данных в БД, также сохраняется событие в таблицу Outbox в той же транзакции - это ключевой момент! Благодаря тому, что запись в Outbox происходит в той же транзакции, что и основная запись, мы гарантируем, что если данные сохранены, то событие о них тоже сохранено. Это решает проблему потери событий при сбоях. Структура таблицы: id, event_type, payload, created_at, processed_at. Отдельный процесс (poller) читает из Outbox и публикует события - фоновый процесс периодически опрашивает таблицу Outbox, находит необработанные события и публикует их в message broker (Kafka, RabbitMQ и т.д.). Этот процесс может быть отдельным сервисом или частью монолита. Интервал polling: 1-5sec, с lock\u0026rsquo;ами для concurrency. После успешной публикации событие помечается как обработанное - только после того, как событие успешно опубликовано в message broker, оно помечается как обработанное. Если публикация не удалась, событие останется в Outbox и будет обработано при следующем опросе. Для cleanup - cron-job удаляет старые записи. graph LR A[Админка] --\u0026gt;|Сохраняет данные| B[(Основная БД)] B --\u0026gt;|В той же транзакции| C[(Outbox таблица)] D[Poller] --\u0026gt;|Читает| C D --\u0026gt;|Публикует| E[Message Broker] E --\u0026gt;|Подписывается| F[Новая система]Плюсы:\nГарантия доставки (транзакционность) - благодаря использованию транзакций БД, мы гарантируем, что событие не потеряется (reliability \u0026gt;99.99%). Минимальные изменения в монолите - нужно только добавить запись в таблицу Outbox при сохранении данных. Остальная логика не меняется. Надежность - даже если poller упал, события останутся в Outbox и будут обработаны после перезапуска; интегрируйте с Kubernetes для auto-restart. Простота понимания - концепция интуитивно понятна, не требует глубокого понимания сложных паттернов. Минусы:\nДополнительная таблица в БД - нужно создать и поддерживать таблицу Outbox, но это небольшая цена за надежность (indexing на processed_at). Нужен отдельный воркер для поллинга outbox таблички - требуется отдельный процесс, который будет опрашивать Outbox. Это может быть узким местом при высокой нагрузке, но можно масштабировать, запуская несколько экземпляров poller\u0026rsquo;а. Eventual consistency - ну а как же без него. События обрабатываются асинхронно, поэтому данные в новой системе появятся с небольшой задержкой. Когда использовать:\nМинимальные изменения в монолите - когда нельзя или нежелательно сильно менять существующий код. Нужна гарантия доставки - когда потеря события недопустима. Простая интеграция - когда нужен быстрый и надежный способ интеграции без сложной инфраструктуры. Когда НЕ использовать:\nКогда нужна синхронизация в реальном времени (миллисекунды). Когда нет возможности добавить таблицу в БД монолита. 6. Change Data Capture (CDC)# Так как я вначале описал 2pc подход, стоит еще рассказать про CDC-подход, когда с помощью базы данных мы отслеживаем изменения и автоматически синхронизируем.\nСуть подхода: Отслеживание изменений в БД монолита и автоматическая синхронизация с новой системой. Это подход \u0026ldquo;нулевого кода\u0026rdquo; - не нужно менять код монолита вообще, достаточно иметь доступ к базе данных. CDC работает на уровне базы данных, отслеживая изменения в transaction log (WAL - Write-Ahead Log). Каждая СУБД ведет лог всех изменений для целей репликации и восстановления. CDC инструменты читают этот лог и преобразуют изменения в события, которые можно отправить в новую систему.\nКак работает:\nCDC инструмент (Debezium, AWS DMS) читает transaction log БД - инструмент подключается к БД (например, Debezium via Kafka Connect для PostgreSQL/MySQL) и читает transaction log в реальном времени. Это не влияет на производительность основной БД, так как чтение лога - это стандартная операция (overhead \u0026lt;1% CPU).\nПреобразует изменения в события - каждое изменение (INSERT, UPDATE, DELETE) преобразуется в событие с информацией о том, что изменилось, когда и в какой таблице. Формат: Avro или JSON, с before/after snapshots.\nПубликует в message broker - события публикуются в Kafka, RabbitMQ или другой message broker для дальнейшей обработки. Интеграция с Schema Registry для эволюции схем.\nНовая система подписывается на события - новая система подписывается на нужные события и обновляет свои данные соответственно. Для фильтрации - конфиг Debezium с table.include.list.\nПример конфига Debezium:\n1{ 2 \u0026#34;name\u0026#34;: \u0026#34;media-connector\u0026#34;, 3 \u0026#34;config\u0026#34;: { 4 \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, 5 \u0026#34;database.hostname\u0026#34;: \u0026#34;old-db-host\u0026#34;, 6 \u0026#34;database.dbname\u0026#34;: \u0026#34;media_db\u0026#34;, 7 \u0026#34;topic.prefix\u0026#34;: \u0026#34;cdc\u0026#34;, 8 \u0026#34;table.include.list\u0026#34;: \u0026#34;public.content,public.media_files\u0026#34; 9 } 10}Метрики: lag \u0026lt;100ms, throughput до 10k событий/sec; мониторьте с Kafka JMX + Grafana.\nПлюсы:\nНулевые изменения в монолите - вообще не нужно трогать код, только настроить CDC инструмент. Реальное время синхронизации - изменения отслеживаются в реальном времени, задержка минимальна (миллисекунды). Автоматическая обработка всех изменений - не нужно помнить публиковать события, все изменения автоматически попадают в новую систему. Надежность (основано на transaction log) - transaction log - это надежный источник истины, который используется самой СУБД для восстановления после сбоев; поддержка snapshot для initial load. Минусы:\nТребуется доступ к БД монолита - нужен доступ к transaction log, что может быть проблемой с точки зрения безопасности. Может быть избыточно для простых случаев - если нужно синхронизировать только несколько таблиц, настройка CDC может быть избыточной. Сложность настройки фильтрации событий - нужно настроить, какие таблицы и колонки отслеживать, чтобы не засорять новую систему ненужными событиями. Зависимость от формата БД - разные СУБД имеют разные форматы transaction log, нужны соответствующие коннекторы. Когда использовать:\nНевозможно изменить монолит - когда код монолита недоступен или его изменение недопустимо. Нужна синхронизация в реальном времени - когда задержка в секунды недопустима. Есть доступ к БД - когда есть возможность подключиться к transaction log БД. Когда НЕ использовать:\nКогда можно изменить код монолита (Outbox проще). Когда нет доступа к БД или transaction log. Когда нужно синхронизировать только часть данных (Outbox проще). Сравнение различных подходов# Для сравнения я выбрал несколько критериев, оцениваю их по 5-балльной шкале, где 1 - низкая сложность/стоимость, а 5 - высокая. Эта таблица поможет быстро сравнить подходы, но помните, что окончательный выбор должен основываться на конкретных требованиях вашего проекта. Я скорректировал оценки для объективности: например, консистентность Saga/Temporal может варьироваться в зависимости от конфигурации.\nКритерий 2PC Saga Temporal Event Sourcing Outbox CDC Стоимость реализации 2 3 4 4 2 3 Сложность разработки 3 4 3 5 2 3 Производительность 2 4 4 4 4 5 Надежность 3 4 5 4 4 4 Масштабируемость 2 5 5 5 4 5 Консистентность 5 3 4 2 2 2 Наблюдаемость 2 3 5 5 3 4 Изменения в монолите 4 3 3 3 2 1 Время доставки 1 3 3 3 3 2 Детальное описание критериев# Дешевизна в реализации\n2PC: Простая концепция, но требует координатора и интеграции с обеими системами. Saga: Нужна инфраструктура для событий (message broker), но сама реализация не очень сложная. Temporal: Требуется развертывание платформы Temporal, что добавляет сложности в инфраструктуру (что аналогично по трудозатратам и рискам использованию брокера сообщений). Event Sourcing: Сложная архитектура - нужно продумать хранение событий, их версионирование. Outbox: Минимальные изменения в монолите, простая реализация - просто таблица и poller. CDC: Требуется специализированный инструмент (Debezium, AWS DMS), но настройка относительно простая. Сложность разработки\n2PC: Средняя сложность - нужно реализовать координатор и интегрировать его с системами. Saga: Высокая - нужно продумать компенсирующие транзакции для каждого шага, что может быть нетривиально. Temporal: Средняя - код workflow пишется просто, но нужно изучить платформу и ее концепции. Event Sourcing: Очень высокая - сложная модель мышления, нужно продумать структуру событий, их версионирование, миграции. Outbox: Низкая - очень простой паттерн, понятный любому разработчику. CDC: Средняя - нужно настроить инструмент и фильтры, но логика обработки событий остается простой. Производительность\n2PC: Низкая - блокировки ресурсов на время транзакции сильно ограничивают пропускную способность. Saga: Высокая - асинхронная обработка позволяет обрабатывать много транзакций параллельно. Temporal: Высокая - платформа оптимизирована для производительности, но есть overhead на управление workflow. Event Sourcing: Высокая - события обрабатываются асинхронно, но может быть overhead на восстановление состояния. Outbox: Высокая - асинхронная обработка, но poller может стать узким местом при очень высокой нагрузке. CDC: Очень высокая - чтение из transaction log очень эффективно, практически без overhead для основной БД. Надежность\n2PC: Средняя - координатор является единой точкой отказа (SPOF), если он упадет в неподходящий момент, транзакция может зависнуть. Saga: Высокая - отказоустойчивость за счет компенсирующих транзакций, но нужно корректно их реализовать. Temporal: Очень высокая - платформа гарантирует выполнение workflow даже при сбоях, это ее основная фича. Event Sourcing: Высокая - можно восстановить состояние через проигрывание событий, но нужно правильно хранить события. Outbox: Высокая - транзакционность гарантирует, что событие не потеряется, даже если poller упадет. CDC: Высокая - основано на transaction log, который является надежным источником истины для самой СУБД. Масштабируемость\n2PC: Низкая - блокировки не позволяют масштабироваться горизонтально, только вертикально. Saga: Очень высокая - каждый сервис может масштабироваться независимо, нет центральных узких мест. Temporal: Очень высокая - платформа спроектирована для масштабирования, можно запускать множество worker\u0026rsquo;ов. Event Sourcing: Очень высокая - события можно обрабатывать параллельно, создавать новые представления данных. Outbox: Высокая - можно запустить несколько экземпляров poller\u0026rsquo;а, но нужно правильно обрабатывать конкурентный доступ. CDC: Очень высокая - можно масштабировать обработчики событий, сам CDC инструмент обычно масштабируется хорошо. Консистентность\n2PC: Строгая (5) - данные в обеих системах всегда синхронизированы, это гарантируется протоколом. Saga: Средняя (3) - eventual, но с возможностью сильной в отдельных шагах via locks. Temporal: Высокая (4) - eventual с гарантиями, configurable. Остальные: Eventual consistency (2) - данные будут синхронизированы, но не мгновенно. Задержка обычно составляет секунды, но может быть и больше при высокой нагрузке или сбоях. Наблюдаемость\n2PC: Низкая - сложно отслеживать состояние распределенных транзакций, особенно при сбоях. Saga: Средняя - можно отслеживать через логи событий, но нужно собирать их из разных сервисов. Temporal: Очень высокая - встроенный UI показывает состояние всех workflow, их историю, можно отлаживать прямо в интерфейсе. Event Sourcing: Очень высокая - полная история всех изменений, можно \u0026ldquo;перемотать\u0026rdquo; время назад и посмотреть состояние на любой момент. Outbox: Средняя - можно отслеживать через логи poller\u0026rsquo;а и состояние таблицы Outbox. CDC: Высокая - CDC инструменты обычно предоставляют метрики и логи о процессе синхронизации. Изменения в монолите\n2PC: Высокие - нужно интегрировать координатор, изменить логику сохранения данных. Saga: Средние - нужно добавить публикацию событий при изменении данных. Temporal: Средние - нужно интегрировать Temporal SDK и изменить логику на workflow. Event Sourcing: Средние - нужно перейти на event-driven модель, что может потребовать рефакторинга. Outbox: Низкие - нужно только добавить запись в таблицу Outbox при сохранении данных, остальная логика не меняется. CDC: Минимальные - вообще не нужно менять код, только дать доступ к БД. Время доставки\n2PC: Синхронная (1) - данные передаются синхронно в рамках транзакции, задержка минимальна (миллисекунды). Остальные: Асинхронные (2-3) - данные передаются асинхронно, задержка обычно составляет секунды. CDC быстрее (ближе к 2), так как читает напрямую из transaction log, остальные медленнее (ближе к 3) из-за дополнительных уровней абстракции. Мониторинг# Метрики# Для каждого подхода необходимо определить ключевые метрики и установить SLA. На мой взгляд будут полезными следующие метрики: Метрики производительности: - Latency (p50, p95, p99) - время от создания события в старой системе до его обработки в новой. Для CDC p95 \u0026lt; 100ms, для Outbox p95 \u0026lt; 5sec, для Saga p95 \u0026lt; 10sec в зависимости от сложности процесса. - Throughput - количество событий в секунду. Измеряйте как на входе (события из монолита), так и на выходе (обработанные события). Разница между ними - это lag, который нужно мониторить. - Error rate - процент событий, которые не удалось обработать. Критический порог обычно 0.1-1% в зависимости от бизнес-требований.\nМетрики надежности: Availability - доступность системы синхронизации. Для критичных систем требуется 99.9% (8.76 часов простоя в год) или выше. Data loss rate - процент потерянных событий. Должен быть 0% для критичных данных, но на практике допускается \u0026lt; 0.01% с механизмами восстановления. Recovery time - время восстановления после сбоя. Целевой показатель MTTR (Mean Time To Recovery) \u0026lt; 15 минут для автоматического восстановления, \u0026lt; 1 часа для ручного вмешательства. Метрики консистентности: Replication lag - задержка между старой и новой системой. Мониторьте как среднее значение, так и максимальное. Для большинства случаев p95 \u0026lt;5sec приемлемо. Reconciliation errors - количество расхождений, обнаруженных при периодической сверке данных. Должно быть 0 для критичных данных, \u0026lt;0.1% для остальных. Дашборды и алертинг# Полезными на мой взгляд будут следующие дашборды:\nОбщий статус синхронизации - количество событий в очереди (в разрезе по типу контента), текущий lag, error rate Исторические тренды - как меняется lag и throughput со временем Детализация по типам событий - отдельные метрики для разных типов данных (текст, видео, аудио) Состояние инфраструктуры - использование CPU/памяти poller\u0026rsquo;ов, размер таблицы Outbox, количество партиций в Kafka Алерты: Критические инциденты: lag \u0026gt;30sec, error rate \u0026gt;1%, недоступность системы \u0026gt;5min Предупреждения: lag \u0026gt;10sec, error rate \u0026gt;0.5%, рост размера очереди \u0026gt;50% за час Информационные: изменения в throughput \u0026gt;20%, увеличение latency p99 \u0026gt;50% Общие проблемы всех подходов# Идемпотентность Это одна из самых важных проблем. Нужно гарантировать, что повторная обработка события не приведет к дублированию данных. Это может произойти, если соблюдается гарантия at-least-once или если обработка была прервана и началась заново. В медиа-системах дубли могут привести к лишним файлам в S3. Решение: использовать ключи идемпотентности или уникальные ключи в БД, которые предотвратят дублирование. Порядок событий В распределенных системах события могут прийти не в том порядке, в котором были созданы. Это особенно актуально при использовании нескольких партиций в Kafka или при параллельной обработке событий. Например, UPDATE может прийти перед INSERT. Решение: использовать версионирование. Каждое событие должно иметь версию или порядковый номер, и обработчик должен проверять, что он обрабатывает события в правильном порядке. Если событие пришло \u0026ldquo;из будущего\u0026rdquo;, его нужно отложить до обработки предыдущих. Обработка ошибок Что делать, если новая система недоступна? Или если данные некорректны? Или если обработка события заняла слишком много времени? В медиа - ошибка при миграции видео может заблокировать процесс. Решение: Retry механизмы с экспоненциальной задержкой (backoff: 1s, 2s, 4s). Использование DQL (Dead letter queue) для событий, которые не удалось обработать после нескольких попыток Компенсирующие транзакции для отката уже выполненных действий. Мониторинг и алертинг для быстрого обнаружения проблем. Схемы данных Старая и новая системы могут иметь разные модели данных. Например, в старой системе поле называется \u0026ldquo;title\u0026rdquo;, а в новой - \u0026ldquo;header\u0026rdquo;. Или в старой системе используется одна структура для хранения адреса, а в новой - другая Решение: можно использовать паттерн адаптер для преобразования данных из одного формата в другой. Специфические проблемы# Каждый подход имеет свои специфические проблемы, о которых важно знать:\nДля Saga:\nСложность реализации компенсирующих транзакций - не всегда легко понять, как откатить действие. Например, как откатить загрузку файла в S3? Нужно продумать все возможные сценарии отката - что делать, если откат сам по себе упал? Для CDC:\nЗависимость от формата БД монолита - разные СУБД имеют разные форматы transaction log. Если монолит использует экзотическую БД, может не быть готового коннектора (Debezium покрывает популярные). Может быть сложно фильтровать нужные изменения - transaction log содержит все изменения во всех таблицах. Нужно настроить фильтры, чтобы обрабатывать только нужные таблицы и колонки. Проблемы с миграцией схемы БД - если схема БД изменилась, нужно обновить конфигурацию CDC. Если миграция была сделана неправильно, можно потерять события или получить некорректные данные. Для Outbox:\nНужен процесс для поллинга изменений в базе данных - poller должен периодически опрашивать таблицу Outbox. При высокой нагрузке это может стать узким местом. Можно решить, запустив несколько экземпляров poller\u0026rsquo;а, но тогда придется обрабатывать конкурентный доступ. Очистка обработанных событий - со временем таблица Outbox будет расти. Нужен процесс для очистки обработанных событий, иначе таблица станет огромной и замедлит работу БД. Практический пример# Исходя из требований (эволюционный переход, минимальные изменения в монолите, надежность), для задачи синхронизации контента между старой и новой системой я бы сейчас выбрал комбинацию Outbox Pattern + Saga с хореографией. Примечание: ранее был выбран другой подход\nПочему именно эта комбинация? Outbox дает надежность доставки событий из монолита с минимальными изменениями, а Saga позволяет обрабатывать сложные сценарии синхронизации (например, когда нужно синхронизировать не только данные, но и файлы) с возможностью отката при ошибках. Это балансирует простоту и надежность.\nOutbox Pattern в монолите для гарантированной публикации событий. Message Broker (Kafka/RabbitMQ) для доставки событий. Adapter Service для трансформации данных. Новая система подписывается на события. graph TB A[Админка монолита] --\u0026gt;|Сохраняет| B[(БД монолита)] B --\u0026gt;|Outbox таблица| C[Outbox Poller] C --\u0026gt;|Публикует| D[Kafka] D --\u0026gt;|Подписывается| E[Adapter Service] E --\u0026gt;|Трансформирует| F[Новая система] F --\u0026gt;|Подтверждение| DПреимущества этого подхода:\nМинимальные изменения в монолите - нужно только добавить запись в таблицу Outbox при сохранении данных. Остальная логика остается без изменений. Надежность доставки - транзакционность гарантирует, что событие не потеряется. Даже если poller упадет, события останутся в Outbox и будут обработаны после перезапуска. Масштабируемость - можно запустить несколько экземпляров poller\u0026rsquo;а и обработчиков событий для параллельной обработки. Возможность добавить дополнительные подписчики - если в будущем понадобится синхронизировать данные с третьей системой, достаточно добавить еще одного подписчика на события. Простота отладки - можно посмотреть состояние таблицы Outbox, логи poller\u0026rsquo;а и обработчиков, чтобы понять, где произошла проблема (используйте Kibana для search). Гибкость - Saga позволяет обрабатывать сложные сценарии, например, когда нужно синхронизировать не только данные, но и файлы, и если загрузка файла не удалась, можно откатить создание записи в БД. Grafana дашборд должен показывать: Количество событий в Outbox по статусам (pending, processed, failed) Latency от создания события до публикации (p50, p95, p99) Throughput: события/сек Error rate по типам ошибок Lag между старой и новой системой Еще раз про риски и потенциальные инциденты# Все предложенные механизмы миграции данных (кроме 2PC) являются асинхронными, соответственно мы должны принять риски, что у нас возможна:\nEventual consistency, Рассинхрон в данных, Соответственно необходимо принять SLA по:\nдопустимому лагу в передаче данных (например, 95% \u0026lt;5sec), пониманию того, как обнаруживать рассинхрон в данных (как явный пример: создали объект, успешно мигрировали. Теперь он доступен в старом и новом интерфейсах обновили объект, но миграция данных не произошла, Теперь в старом интерфейсе объект более новой версии, чем в новом интерфейсе пониманию того, что считается инцидентом (lag \u0026gt;30sec или loss rate \u0026gt;0.1%). Что дальше# После того, как механизм миграции успешно имплементирован, можно переходить к следующей фазе - переделке write-side части. После переделки write-side части интеграция между старой и новой системой становится не нужной, так как новая система уже включает в себя как read-side, так и write-side. Соответственно можно будет выключить рубильник, погасить монолит, убрать механизм миграции и наслаждаться работой с новой системой.\nЗаключение# Переход с монолита на микросервисы - это не разовое событие, а длительный процесс, который может занимать месяцы или даже годы. Выбор подхода для синхронизации данных - это один из ключевых решений на этом пути, и от него зависит успех всей миграции. В одном известном кейсе миграция заняла годы, но выбор подходящего подхода сделал переход плавным.\nЯ описал механизмы передачи данных между двумя системами, но нужно понимать, что это временное решение переходного этапа. Попытка зафиксировать любую из предложенных архитектур как целевую повлечет за собой рост сложности, увеличение стоимости поддержки и потенциально команда может скатиться только в лечение локальных проблем и \u0026ldquo;тушение пожаров\u0026rdquo;.\nВыбор подхода для синхронизации данных зависит от множества факторов: требований к консистентности, производительности, сложности реализации и ограничений существующей системы. Не существует универсального решения, которое подходит для всех случаев. Каждый проект уникален и требует индивидуального подхода.\nНужно помнить, что идеального решения не существует, есть только компромиссы.\nМатериалы# Event Sourcing Pattern Saga Pattern Статья на Habr о распределенных транзакциях Distributed Transaction Patterns for Microservices Compared Two-Phase Commit Outbox Pattern Temporal.io Documentation Debezium Documentation Mark Richards on Software Architecture "}]